

**Service Mesh 是微服务时代的 TCP/IP 协议。**

service mesh

[Phil Calçado](https://link.zhihu.com/?target=http%3A//philcalcado.com/)的文章[《Pattern: Service Mesh》](https://link.zhihu.com/?target=http%3A//philcalcado.com/2017/08/03/pattern_service_mesh.html)

https://zhuanlan.zhihu.com/p/61901608

https://blog.csdn.net/baichoufei90/article/details/107293203

service mesh 的问题与尝试性解决方案

https://juejin.cn/post/6844903897065078792

# [微服务](https://so.csdn.net/so/search?q=微服务&spm=1001.2101.3001.7020)架构的核心技术问题

在微服务模式下，企业内部服务少则几个到几十个，多则上百个，每个服务一般都以集群方式部署，这时自然产生两个问题：

1. 服务发现
2. 负载均衡

而解决这问题目前业界的成熟方案核心均是代理

## 模式一：传统集中式代理

​														![传统集中式代理](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206052346766.png)
这是最简单和传统做法，在服务消费者和生产者之间，代理作为独立一层集中部署，由独立团队(一般是运维或框架)负责治理和运维。常用的集中式代理有硬件负载均衡器(如F5)，或者软件负载均衡器(如Nginx)，F5(4层负载)+Nginx(7层负载)这种软硬结合两层代理也是业内常见做法，兼顾配置的灵活性(Nginx比F5易于配置)。

这种方式通常在DNS域名服务器的配合下实现服务发现，服务注册(建立服务域名和IP地址之间的映射关系)一般由运维人员在代理上手工配置，服务消费方仅依赖服务域名，这个域名指向代理，由代理解析目标地址并做负载均衡和调用。

国外知名电商网站eBay，虽然体量巨大，但其内部的服务发现机制仍然是基于这种传统的集中代理模式，国内公司如携程，也是采用这种模式。

我们项目的主应用中负载均衡更多的是依赖k8s提供的ingress和service提供的负载均衡，nginx更多的是提供一个动静分离、反向代理和域名重定向的作用

## 模式二：客户端嵌入式代理

​															![客户端嵌入式代理](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206052347288.png)
这是很多互联网公司比较流行的一种做法，代理(包括服务发现和负载均衡逻辑)以客户库的形式嵌入在应用程序中。这种模式一般需要独立的服务注册中心组件配合，服务启动时自动注册到注册中心并定期报心跳，客户端代理则发现服务并做负载均衡。

Netflix开源的Eureka(注册中心)[附录1]和Ribbon(客户端代理)[附录2]是这种模式的典型案例，国内阿里开源的Dubbo也是采用这种模式。

现在用到的框架里sf-boot默认为springcloud的feign，feign相对于dubbo的优点在于Feign追求的是简洁，少侵入，但是对于整个服务治理的流程和应对各种场景提供的rpc效率都相较于SOA核心服务治理框架的Dubbo要逊色不少



##  模式三：主机独立进程代理

这种做法是上面两种模式的一个折中，代理既不是独立集中部署，也不嵌入在客户应用程序中，而是作为独立进程部署在每一个主机上，一个主机上的多个消费者应用可以共用这个代理，实现服务发现和负载均衡，如下图所示。这个模式一般也需要独立的服务注册中心组件配合，作用同模式二。

​																![主机独立进程代理](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206052348811.png)
Airbnb的SmartStack是这种模式早期实践产品，国内公司唯品会、携程对这种模式也有探索和实践。



# 三种服务发现模式的比较

上面介绍的三种服务发现模式各有优劣，没有绝对的好坏，可以认为是三种不同的架构风格，在不同的公司都有成功实践。下表总结三种服务发现模式的优劣比较，业界案例和适用场景建议，供[架构师](https://so.csdn.net/so/search?q=架构师&spm=1001.2101.3001.7020)选型参考：

![在这里插入图片描述](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206052350993.png)



# 服务网格ServiceMesh

所谓的ServiceMesh，其实本质上就是上面提到的模式三~主机独立进程模式，这个模式其实并不新鲜，业界(国外的Airbnb和国内的唯品会等)早有实践，那么为什么现在这个概念又流行起来了呢？我认为主要原因如下：

- 上述模式一和二有一些固有缺陷，模式一相对比较重，有单点问题和性能问题；模式二则有客户端复杂，支持多语言困难，无法集中治理的问题。模式三是模式一和二的折中，弥补了两者的不足，它是纯分布式的，没有单点问题，性能也OK，应用语言栈无关，可以集中治理。
- 微服务化、多语言和容器化发展的趋势，企业迫切需要一种轻量级的服务发现机制，ServiceMesh正是迎合这种趋势诞生，当然这还和一些大厂(如Google/IBM等)的背后推动有关。
- 将原本各家混乱的逻辑抽象成了各个维度需要的概念

模式三(ServiceMesh)也被形象称为边车(Sidecar)模式，如下图，早期有一些摩托车，除了主驾驶位，还带一个边车位，可以额外坐一个人。在模式三中，业务代码进程(相当于主驾驶)共享一个代理(相当于边车)，代理除了负责服务发现和负载均衡，还负责动态路由、容错限流、监控度量和安全日志等功能，这些功能是具体业务无关的，属于跨横切面关注点(Cross-Cutting Concerns)范畴。

![在这里插入图片描述](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206052351842.png)

# 演化过程

**时代0**：开发人员想象中，不同服务间通信的方式，抽象表示如下：

![image-20220529234925457](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205292349476.png)

**时代1：原始通信时代**

然而现实远比想象的复杂，在实际情况中，通信需要底层能够传输字节码和电子信号的物理层来完成，在TCP协议出现之前，服务需要自己处理网络通信所面临的丢包、乱序、重试等一系列流控问题，因此服务实现中，除了业务逻辑外，**还夹杂着对网络传输问题的处理逻辑。**（服务自己对操作系统提供的网络通信接口的异常进行处理）

![image-20220529235932952](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205300000244.png)

**时代2：TCP时代**

为了避免每个服务都需要自己实现一套相似的网络传输处理逻辑，TCP协议出现了，它解决了网络传输中通用的流量控制问题，将技术栈下移，从服务的实现中抽离出来，成为操作系统网络层的一部分。

![image-20220529235900576](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205292359596.png)

**时代3：第一代微服务**

在TCP出现之后，机器之间的网络通信不再是一个难题，以GFS/BigTable/MapReduce（和以其为基础hadoop）为代表的分布式系统得以蓬勃发展。这时，分布式系统特有的通信语义又出现了，如熔断策略、负载均衡、服务发现、认证和授权、quota限制、trace和监控等等，于是服务根据业务需求来实现一部分所需的通信语义。																
																			![image-20220530195950302](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205301959325.png)	

**时代4：第二代微服务**

为了避免每个服务都需要自己实现一套分布式系统通信的语义功能，随着技术的发展，一些面向微服务架构的开发框架出现了，如Twitter的[Finagle](https://link.zhihu.com/?target=https%3A//finagle.github.io/)、Facebook的[Proxygen](https://link.zhihu.com/?target=https%3A//code.facebook.com/posts/1503205539947302)以及Spring Cloud等等，这些框架实现了分布式系统通信需要的各种通用语义功能：如负载均衡和服务发现等，因此一定程度上屏蔽了这些通信细节，使得开发人员使用较少的框架代码就能开发出健壮的分布式系统。

第一代和第二代的区别大致是：Dubbo是一个分布式服务框架,是阿里巴巴SOA服务化治理方案的核心框架;Hadoop是一个能够对大量数据进行分布式处理的软件框架，也可以说是一个大数据的生态系统

![image-20220530204347209](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205302044660.png)



**时代5：第一代Service Mesh**

第二代微服务模式看似完美，但开发人员很快又发现，它也存在一些本质问题：

- 其一，虽然框架本身屏蔽了分布式系统通信的一些通用功能实现细节，但开发者却要花更多精力去掌握和管理复杂的框架本身，在实际应用中，去追踪和解决框架出现的问题也绝非易事；(出现问题修复时，学习成本高)
- 其二，开发框架通常只支持一种或几种特定的语言，回过头来看文章最开始对微服务的定义，一个重要的特性就是语言无关，但那些没有框架支持的语言编写的服务，很难融入面向微服务的架构体系，想因地制宜的用多种语言实现架构体系中的不同模块也很难做到（框架本身的使用只能支持特定语言的版本,若需要通过治理的信息，别的语言只能通过http或者其他协议与公共服务治理服务通信，不合理）；
- 其三，框架以lib库的形式和服务联编，复杂项目依赖时的库版本兼容问题非常棘手，同时，框架库的升级也无法对服务透明，服务会因为和业务无关的lib库升级而被迫升级；（版本升级时的依赖问题）

因此以Linkerd，Envoy，NginxMesh为代表的代理模式（边车模式）应运而生，这就是第一代Service Mesh，它将分布式服务的通信抽象为单独一层，在这一层中实现负载均衡、服务发现、认证授权、监控追踪、流量控制等分布式系统所需要的功能，作为一个和服务对等的代理服务，和服务部署在一起，接管服务的流量，通过代理之间的通信间接完成服务之间的通信请求，这样上边所说的三个问题也迎刃而解。

![image-20220530235001220](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205302350245.png)

如果我们从一个全局视角来看，就会得到如下部署图：

![image-20220530235112548](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205302351569.png)

如果我们暂时略去服务，只看Service Mesh的单机组件组成的网络：

![image-20220530235148620](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205302351641.png)

***时代6：第二代Service Mesh**

第一代Service Mesh由一系列独立运行的单机代理服务构成，为了提供统一的上层运维入口，演化出了集中式的控制面板，所有的单机代理组件通过和控制面板交互进行网络拓扑策略的更新和单机数据的汇报。这就是以Istio为代表的第二代Service Mesh。

在新一代的ServiceMesh架构中(下图上方)，服务的消费方和提供方主机(或者容器)两边都会部署代理SideCar。ServiceMesh比较正式的术语也叫数据面板(DataPlane)，与数据面板对应的还有一个独立部署的控制面板(ControlPlane)，用来集中配置和管理数据面板，也可以对接各种服务发现机制(如K8S服务发现)。

![image-20220530235331845](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205302353870.png)

只看单机代理组件(数据面板)和控制面板的Service Mesh全局部署视图如下：

![image-20220530235344421](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202205302353442.png)

再回过头来看Buoyant的CEO William Morgan，也就是Service Mesh这个词的发明人，对Service Mesh的定义：

> 服务网格是一个***基础设施层\***，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证***请求在这些拓扑中可靠地穿梭\***。在实际应用当中，服务网格通常是由一系列轻量级的***网络代理\***组成的，它们与应用程序部署在一起，但***对应用程序透明\***。

这个定义中，有四个关键词：

**基础设施层**+**请求在这些拓扑中可靠穿梭**：这两个词加起来描述了Service Mesh的定位和功能，是不是似曾相识？没错，你一定想到了TCP；

**网络代理**：这描述了Service Mesh的实现形态；

**对应用透明**：这描述了Service Mesh的关键特点，正是由于这个特点，Service Mesh能够解决以Spring Cloud为代表的第二代微服务框架所面临的三个本质问题；

## 两代Service Mesh介绍

第一代Service Mesh的代表为Linkerd和Envoy。Linkerd基于Twitter的Fingle，使用Scala编写，是业界第一个开源的Service Mesh方案，在长期的实际生产环境中获得验证。Envoy底层基于C++，性能上优于使用Scala的Linkerd。同时，Envoy社区成熟度较高，商用稳定版本面世时间也较长。这两个开源实现都是以Sidecar为核心，绝大部分关注点都是如何做好Proxy，并完成一些通用控制面的功能。但是当你在容器中大量部署Sidecar以后，如何管理和控制这些Sidecar本身就是一个不小的挑战。
																![在这里插入图片描述](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206060003591.png)

第二代Service Mesh主要改进集中在更加强大的控制面功能（与之对应的Sidecar Proxy被称之为数据面），典型代表有Istio和Conduit。Istio是Google、IBM和Lyft合作的开源项目，是目前最主流的Service Mesh方案，也是事实上的第二代Service Mesh标准。在Istio中，直接把Envoy作为Sidecar。除了Sidecar，Istio中的控制面组件都是使用Go语言编写。



## Istio

​												![在这里插入图片描述](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206060004270.png)	
Istio是Google/IBM等大厂支持和推进的一个ServiceMesh标准化工作组，上图是Istio给出的ServiceMesh参考架构。

Istio专注在控制面板的架构、功能、以及控制面板和数据面板之间API的标准化，它的控制面板功能主要包括：

- Istio-Manager
  负责服务发现，路由分流，熔断限流等配置数据的管理和下发

- Mixer
  负责收集代理上采集的度量数据，进行集中监控（很多Servicemesh的实现都摒弃或者简化了Mixer）

- Istio-Auth
  负责安全控制数据的管理和下发

  提供强大的服务间认证和终端用户认证，使用交互TLS，内置身份和证书管理。可以升级服务网格中的未加密流量，并为运维人员提供基于服务身份而不是网络控制来执行策略的能力。Istio的未来版本将增加细粒度的访问控制和审计，以使用各种访问控制机制（包括基于属性和角色的访问控制以及授权钩子）来控制和监视访问服务、API或资源的访问者。

Istio的设计理念先进，功能也比较强大，加之Google、IBM的影响力让Istio迅速传播，让广大开发者认识到了Istio项目在Service Mesh领域的重要性。但是Istio目前版本也存在了一些不足：

目前的**Istio大部分能力与Kubernetes是强关联的**。而我们在构建微服务的时候往往是希望服务层与容器层是解耦的，服务层在设计上需要能够对接多种容器层平台。
其他主要组件：

- Envoy
  扮演Sidecar的功能，协调服务网格中所有服务的出入站流量，并提供服务发现、负载均衡、限流熔断等能力，还可以收集与流量相关的性能指标。

  Envoy是目前Istio主力支持的数据面板代理，其它主流代理如nginx/kong等也正在陆续加入这个阵营。kubernetes是目前Isito主力支持的容器云环境。

- Pilot
  负责部署在Service Mesh中的Envoy实例的生命周期管理。本质上是负责流量管理和控制，将流量和基础设施扩展解耦，这是Istio的核心。可以把Pilot看做是管理Sidecar的Sidecar, 但是这个特殊的Sidacar并不承载任何业务流量。Pilot让运维人员通过Pilot指定它们希望流量遵循什么规则，而不是哪些特定的pod/VM应该接收流量。有了Pilot这个组件，我们可以非常容易的实现 A/B 测试和金丝雀Canary测试。



## Conduit

我们再来看一下Conduit的实现，下图是Conduit的架构设计图，其中重点由Conduit Data Plane和Conduit Control Plane两部分组成：
													![在这里插入图片描述](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206060004027.png)
Conduit各方面的设计理念与Istio非常类似，作者使用Rust语言重新编写了Sidecar, 叫做Conduit Data Plane, 控制面则由Go语言编写的Conduit Control Plane接管。从Conduit的架构看，作者号称Conduit吸取了很多Linkerd的教训，比Linkerd更快、更轻、更简单，控制面功能更强。与Istio比较，Conduit的架构一方面比较简单，另一方面对于要解决的问题足够聚焦。

ServiceMesh 一些落地的问题，主要原因如下：

- ServiceMesh其实并不是什么新东西，本质就是上面提到的服务发现模式三~主机独立进程模式，这个模式很早就有公司在探索和实践，但是一直没有普遍流行起来，说明这个模式也是存在落地挑战的。从表面上看，模式三是模式一和模式二的折中，同时解决了模式一和模式二存在的问题，但是在每个主机上独立部署一个代理进程，是有很大运维管理开销的，一方面是规模化部署的问题(考虑服务很多，机器也很多的场景)；另一方面是如何监控治理的问题，代理挂了怎么办？你的团队是否具备自动化运维和监控的能力？另外开发人员在服务调试的时候，会依赖于这个独立的代理，调试排错比较麻烦，这个问题怎么解决？
- Istio的确做了一些标准化工作，但是没有什么特别的创新，可是说换汤不换药，就是把模式三规范化和包装了一下。透过现象看本质，Google/IBM等行业大厂在背后推Isito/ServiceMesh，背后有一些市场利益诉求考虑，例如Google要推进它的kubernates和公有云生态，对于公司的云生态的底层和k8s的使用，若是掌控程度不足，也值得思考。
- ServiceMesh在国内大产的实现上如腾讯的北极星在内的等在内也在探索（也多数基于envoy和istio开发拓展），开荒需要的是极大的人力物力，并且不一定能成功，但是每次基于所有服务整体的降本增效都会有长远的好处，如亚马逊的所有服务加入SOA治理。

总结一下，Service Mesh具有如下优点：

- 屏蔽分布式系统通信的复杂性(负载均衡、服务发现、认证授权、监控追踪、流量控制等等)，服务只用关注业务逻辑；
- 真正的语言无关，服务可以用任何语言编写，只需和Service Mesh通信即可；
- 对应用透明，Service Mesh组件可以单独升级；



当然，Service Mesh目前也面临一些挑战（猫眼技术团队的思考,19年的）：

## 资源上的损耗

> mesh本质上相当于寄生在业务机器上。使用的是业务机器的资源。实际上的测试中发现，由于采用了c++/go实现的mesh对于内存的消耗比较可控，默认情况下只占用几M，在高并发下一般也只会上升至几十M。这对于正常情况下8G/16G内存的应用机器来说基本可以忽略不计。所以内存的额外占用这个问题可以基本忽略。但是其对于cpu资源的消耗则较大，一般会趋近于业务正常使用的cpu资源量。这意味着，加入了mesh之后，有可能业务能使用的cpu资源只剩下本来的一半。这就是一个比较大的问题了。
>
> 会衍生出的两个新问题：
> **资源不会无限期闲置**
>
> **资源使用率这个数字之外，还有业务高低峰问题**。
>
> 使用Server Proxy作为闲置资源不够时的兜底方案，采取**逻辑上**的Central Mesh来解决上述的问题：
>
> 1. Sidecar进行闲置资源探测
> 2. 当发现闲置资源即将不足，则告知sdk切换流量至Central Mesh
> 3. Central Mesh完成Sidecar所有的工作。
>
> Central Mesh装载有所在区域所需的的所有信息，承担起Sidecar的所有能力。即Central Mesh也作为所在区域Sidecar的backup，在Sidecar失灵或者闲置资源不足以正常运转Sidecar的情况下，主动切换流量。

![img](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206091117179.webp)

## 性能上的损耗

性能上的损耗是回避不了的一个问题。由于多进行了一次转发以及要进行服务治理，所以性能天然地比直连RPC的方式的性能要差。基于我们实际的性能测试结果，其相比于直连，mesh方式的性能会退化20-50%左右，这还是在不采用iptables这种更耗性能的方式下进行的测试。当然，这个增加的延时在毫秒级，对于大部分的业务要求来说，其实是可接受的。对业务性能的影响微乎其微。

潜在问题：

1. **业务应用问题。**对于一些高并发的业务场景，可能本身延时就低（毫秒级），且对延时敏感，加上一次调用链路可能会有七八次甚至十次以上的RPC调用，如果以这种方式进行改造，则可能导致这类业务性能退化严重，甚至可能引起比如超时、线程池打满等问题。
2. **基础应用问题。**如果servicemesh的未来趋势是所有通讯流量都mesh化而不仅仅是业务应用的流量，那么我们就还需要考虑，比如类似redis这样的对延时极度敏感的存储流量进行mesh化后，我们对于mesh带来额外延时的忍耐度将进一步降低。redis本身就是一个超高并发、极度低延时、非常延时敏感的场景。多出一毫秒的延时都很有可能会引起Redis可用性的成倍下滑甚至引起业务故障。

所以针对以上问题，一方面，我们需要对于性能的退化有心理预期，另一方面，我们也应该竭尽所能地优化甚至压榨servicemesh的性能极限，而不是说选择了mesh就放弃了性能听之任之了。想想netty著名的压榨性能到极致的“eventloop挑选”。

在mesh的通讯性能优化上，有几个可以考虑的点：

1. **本地进程通讯优化**。mesh由于和业务进程在同一机器上，具备利用本地进程通讯加速通讯性能的前提条件。本地进程通讯存在多种方式，比如mmap、unix domain socket、pipe、signal等等。其中又以mmap性能最为突出，traffic-shm 是一个异步无锁的IPC框架，能够轻松支撑百万 TPS，其就采用了 mmap 来进行通讯。通过实际测试，采用mmap结合适当的事件通知机制，在某些高并发的场景上，其性能相较于tcp的方式会提升30%以上。
2. **线程模型**。基本高性能服务的底层都采用了Reactor模式来实现线程模型。当然，配合线程池/协程池，以及 Reactor 的层次，可以有多种的实现路径。有类似 Nginx 这样的一主多子进程 + 单 Reactor 单线程（高版本提供了线程池机制）的模型，evio 和 Envoy 利用“单 Reactor 协程池（线程池）”， Netty 采用多级 Reactor + 多级线程池。避免mesh出现阻塞式设计。
3. **字节重用**。我们习惯于对每个请求去新创建一个他所需要的空间来存放一些信息。但是当并发量上来后，这样的空间分配将会导致较大的性能开销和回收压力。所以基于伙伴算法或者Slab算法或其他的方式，进行一个字节内存使用的分配管理，将会让你收益良多。比如 Netty 由于申请了堆外内存而采用了伙伴算法，Nginx 则采用了 Slab 机制，Mosn 在 Golang 分配机制的基础上引入多级容量加 sync.Pool 机制的缓存来进行优化。
4. **内存对齐**。操作系统按照页进行内存管理。如果你直接操作内存地址进行数据传输（比如用 mmap）的话，那么如果没有内存对齐，将会导致你拉取到并不需要的内存空间，且会有内存移动拼接的额外开销，这将会直接导致你性能的下滑。高性能内存队列 Disruptor 也是采用了内存对齐的方式进行优化。
5. **无锁化**。通讯的第一反应即需要处理并发安全的问题，很多时候你可能不得不通过锁的方式来保障安全。这个时候，考虑下通过利用硬件层面的CAS操作来替代常规的锁操作，也可以采取类似 Redis 这样单线程处理的方式，或者 Envoy 这样虽然采用了线程池，但是连接会进行单个线程的绑定操作来规避并发问题。
6. **池化**。线程资源是很宝贵的，加上线程本身不可能申请几千上万个出来，所以线程池是默认的标配。这边要谈的是，比如你用了协程技术，虽然协程被神化为轻量级线程，性能非常高且可以轻松开出几万个而面不改色。但是你需要知道，这也会很严重地影响你实际的处理性能，并且由于 golang 本身的协程分配原理，协程的一些元数据并不会在使用后被回收，这是由于 golang 开发者的理念是“一旦到达过这样的流量，就说明系统有可能再次面临这样的洪峰，那么就提前做好准备”。所以我们对于协程，仍然需要考虑池化的问题。Motan-Go 和 Thrift 的 golang 版本中目前并没有这方面的考虑，而 Sofa-Mosn 已经做了对应的池化处理。

## Sidecar功能的相互影响

- 比如，如何确保日志采集这种大规模但不重要的流量，完全不会影响核心业务流量？

- 比如，如何确保某个功能的升级，不影响核心业务通讯？
- 比如，如何多个团队一起维护一个Sidecar？

拆解sidecar，Sofa-mosn已经拆出了单独的dbmesh。

历史总是惊人的相似。为了解决端到端的字节码通信问题，TCP协议诞生，让多机通信变得简单可靠；微服务时代，Service Mesh应运而生，屏蔽了分布式系统的诸多复杂性，让开发者可以回归业务，聚焦真正的价值。

## 只负责服务订阅不负责服务发布？

我们说过service mesh 会取代现在微服务间的通信框架，那么原先服务治理中的服务发现功能在原先主机独立进程代理中只负责了服务订阅的功能，为Servicemesh是在云原生背景下基于Local Proxy所演进而来的。而Local Proxy在云原生方案里面，基本都不会负责服务注册，因为他们会将服务注册交给云（Mesos、Marathon、K8s等都有现成方案）来实现，或者会一般会结合consul/etcd/zk单独另起agent来完成服务注册。此时Local Proxy则朴素地只关注反向代理的工作。

将整个将服务治理mesh化，反而把整套服务订阅发布体系迁移到云原生体系中去，那就有点本末倒置了。所以就必然会选择进行适配。Sidecar将加入服务注册的能力，再让Sidecar对接第三方的注册中心。

所以笔者认为Servicemesh是需要彻底屏蔽掉具体注册中心的存在的。发布和订阅应该都经过Pilot，为使用方提供统一门面。以后无论注册中心如何切换，都无需再深度侵入修改Sidecar。



![img](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206091126547.webp)

## 控制面板和数据面板如何切割？

这是一个老大难的问题。控制面板中的Mixer现在基本是处于一个“墙倒众人推”的地步。Istio比较偏执地将其单拎出来，处理限流和数据遥测等事宜。前者会带来很大的性能瓶颈（即使Istio后续在Envoy中增加了缓存机制也无力回天），后者会带来双倍的流量消耗。很多Servicemesh的实现都摒弃或者简化了Mixer。

虽然Istio的这一设计太过于理想化，希望通过这种方式屏蔽基础设施差异性，然后为Sidecar提供无限庞大的内存容量支持，同时将一些复杂多变逻辑尽可能剔除出Sidecar，来保证Sidecar尽可能稳定可靠。但是现实还是很残酷的，有通讯的地方就会产生问题。分布式环境的复杂性大半就是由于网络导致的。

而如果将Mixer一股脑地下沉，则不得不面对各种复杂逻辑下如何保障Sidecar本身足够稳定可靠，消耗足够少资源，引入尽量少依赖，足够小而美的状态？

虽然控制面试和数据面板的切割是一个很困难的命题，而Istio这方面也引起了一些吐槽，但是从先驱者的角度上来看，Istio成功整合了发展了很长时间的Local Proxy方案，并将其上升到了方法论的高度，成功引发了业内对于控制面板和数据面板的体系化思考，这才是Istio做出的最大贡献，这是一场从战术到战略的变革，从术到道的创新。

## 各大公司的实现方案

巨头合作的 Istio（其本来目的是为了帮助应用更好地上云）的珠玉在前，其他公司也纷纷基于/参考 Isito 做出自己的解决方案：

1. 阿里基于 golang 重写了 Envoy 并在这基础上构建了 Sofa-Mosn/Pilot，下沉了 Istio 中颇受诟病的 Mixer 的限流能力；
2. 腾讯基于 Envoy 进行改造整合内部的 TSF 服务框架；
3. 微博基于 Motan-Go 研制出 Motan-Mesh，整合了自己的服务治理体系；
4. 华为的 ServiceComb 也是类似的做法， Mixer完全下沉；
5. Twitter推出 Conduit，基于Rust，也将Mixer完成下沉；
6. 北斗借鉴istio，mosn，polaris，在sidecar中支持注册订阅，并将Mixer下沉

# Serverless

Serverless被翻译为“无服务器架构”，这个概念在2012年时便已经存在，比微服务和Service Mesh的概念出现都要早，但是直到微服务概念大红大紫之后，Serverless才重新又被人们所关注。

Serverless（无服务器架构）并不意味着没有任何服务器去运行代码，Serverless是无需管理服务器，只需要关注代码，而提供者将处理其余部分工作。

“无服务器架构”也可以指部分服务器端逻辑依然由应用程序开发者来编写的应用程序，但与传统架构的不同之处在于，这些逻辑运行在完全由第三方管理，由事件触发的无状态（Stateless）暂存于计算容器内。

对于开发者来说，Serverless架构可以将其服务器端应用程序分解成多个执行不同任务的函数，整个应用分为几个独立、松散耦合的组件，这些组件可以在任何规模上运行。

下图为一种常见的Serverless架构图，所有的服务都以FaaS（函数即服务）的方式对外进行提供。在语言和环境方面，FaaS 函数就是常规的应用程序，例如使用JavaScript、Python以及 Java等语言实现。

![在这里插入图片描述](https://raw.githubusercontent.com/eternal-heathens/picgoBeg/master/JavaImages/202206060036064.png)





## Serverless计算是什么

云计算涌现出很多改变传统IT架构和运维方式的新技术，比如虚拟机、容器、微服务，无论这些技术应用在哪些场景，降低成本、提升效率是云服务永恒的主题。

过去十年来，我们已经把应用和环境中很多通用的部分变成了服务。Serverless的出现，带来了跨越式变革。Serverless把主机管理、操作系统管理、资源分配、扩容，甚至是应用逻辑的全部组件都外包出去，把它们看作某种形式的商品——厂商提供服务，我们掏钱购买。

过去是“构建一个框架运行在一台服务器上，对多个事件进行响应”，Serverless则变为“构建或使用一个微服务或微功能来响应一个事件”，做到当访问时，调入相关资源开始运行，运行完成后，卸载所有开销，真正做到按需按次计费。这是云计算向纵深发展的一种自然而然的过程。

Serverless是一种构建和管理基于微服务架构的完整流程，允许你在**服务部署级别**而不是服务器部署级别来管理你的应用部署。

它与传统架构的不同之处在于，完全由第三方管理，由事件触发，存在于无状态（Stateless）、暂存（可能只存在于一次调用的过程中）计算容器内。构建无服务器应用程序意味着开发者可以专注在产品代码上，而无须管理和操作云端或本地的服务器或运行时。Serverless真正做到了部署应用无需涉及基础设施的建设，自动构建、部署和启动服务。

国内外的各大云厂商 Amazon、微软、Google、IBM、阿里云、腾讯云、华为云相继推出Serverless产品，Serverless也从概念、愿景逐步走向落地，在各企业、公司应用开来。

## FaaS和BaaS

##  FaaS（Function as a Service，函数即服务）

FaaS意在无须自行管理服务器系统或自己的服务器应用程序，即可直接运行后端代码。其中所指的服务器应用程序，是该技术与容器和PaaS（平台即服务）等其他现代化架构最大的差异。

FaaS可以取代一些服务处理服务器（可能是物理计算机，但绝对需要运行某种应用程序），这样不仅不需要自行供应服务器，也不需要全时运行应用程序。

FaaS产品不要求必须使用特定框架或库进行开发。在语言和环境方面，FaaS函数就是常规的应用程序。例如AWS Lambda的函数可以通过Javascript、Python以及任何JVM语言（Java、Clojure、Scala）等实现。然而Lambda函数也可以执行任何捆绑有所需部署构件的进程，因此可以使用任何语言，只要能编译为Unix进程即可。FaaS函数在架构方面确实存在一定的局限，尤其是在状态和执行时间方面。

在迁往FaaS的过程中，唯一需要修改的代码是“主方法/启动”代码，其中可能需要删除顶级消息处理程序的相关代码（“消息监听器接口”的实现），但这可能只需要更改方法签名即可。在FaaS的世界中，代码的其余所有部分（例如向数据库执行写入的代码）无须任何变化。

相比传统系统，部署方法会有较大变化 – 将代码上传至FaaS供应商，其他事情均可由供应商完成。目前这种方式通常意味着需要上传代码的全新定义（例如上传zip或JAR文件），随后调用一个专有API发起更新过程。

FaaS中的函数可以通过供应商定义的事件类型触发。对于亚马逊AWS，此类触发事件可以包括S3（文件）更新、时间（计划任务），以及加入消息总线的消息（例如Kinesis）。通常你的函数需要通过参数指定自己需要绑定到的事件源。

大部分供应商还允许函数作为对传入Http请求的响应来触发，通常这类请求来自某种该类型的API网关（例如AWS API网关、Webtask）。

###  BaaS（Backend as a Service，后端即服务）

BaaS（Backend as a Service，后端即服务）是指我们不再编写或管理所有服务端组件，可以使用领域通用的远程组件（而不是进程内的库）来提供服务。理解BaaS，需要搞清楚它与PaaS的区别。

首先BaaS并非PaaS，它们的区别在于：PaaS需要参与应用的生命周期管理，BaaS则仅仅提供应用依赖的第三方服务。典型的PaaS平台需要提供手段让开发者部署和配置应用，例如自动将应用部署到Tomcat容器中，并管理应用的生命周期。BaaS不包含这些内容，BaaS只以API的方式提供应用依赖的后端服务，例如数据库和对象存储。BaaS可以是公共云服务商提供的，也可以是第三方厂商提供的。其次从功能上讲，BaaS可以看作PaaS的一个子集，即提供第三方依赖组件的部分。

BaaS服务还允许我们依赖其他人已经实现的应用逻辑。对于这点，认证就是一个很好的例子。很多应用都要自己编写实现注册、登录、密码管理等逻辑的代码，而对于不同的应用这些代码往往大同小异。完全可以把这些重复性的工作提取出来，再做成外部服务，而这正是Auth0和Amazon Cognito等产品的目标。它们能实现全面的认证和用户管理，开发团队再也不用自己编写或者管理实现这些功能的代码。

### 无服务器（Serverless）计算如何工作？

与使用虚拟机或一些底层的技术来部署和管理应用程序相比，无服务器计算提供了一种更高级别的抽象。因为它们有不同的抽象和“触发器”的集合。

拿计算来讲，这种抽象有一个特定函数和抽象的触发器，它通常是一个事件。以数据库为例，这种抽象也许是一个表，而触发器相当于表的查询或搜索，或者通过在表中做一些事情而生成的事件。

比如一款手机游戏，允许用户在不同的平台上为全球顶级玩家使用高分数表。当请求此信息时，请求从应用程序到API接口。API接口或许会触发AWS的Lambda函数，或者无服务器函数，这些函数再从数据库表中获取到数据流，返回包含前五名分数的一定格式的数据。

一旦构建完成，应用程序的功能就可以在基于移动和基于 Web 的游戏版本中重用。

这跟设置服务器不同，不是必须要有Amazon EC2实例或服务器，然后等待请求。环境由事件触发，而响应事件所需的逻辑只在响应时执行。这意味着，运行函数的资源只有在函数运行时被创建，产生一种非常高效的方法来构建应用程序。

## 无服务器（Serverless）适用于哪些场景？

在现阶段，Serverless主要应用在以下几个场景

- 首先在Web及移动端服务中，可以整合API网关和Serverles服务构建Web及移动后端，帮助开发者构建可弹性扩展、高可用的移动或 Web后端应用服务。
- 在IoT场景下可高效的处理实时流数据，由设备产生海量的实时信息流数据，通过Serverles服务分类处理并写入后端处理。
- 另外在实时媒体资讯内容处理场景里，用户上传的音视频到对象存储OBS，通过上传事件触发多个函数，分别完成高清转码、音频转码等功能，满足用户对实时性和并发能力的高要求。
- 无服务器计算还适合于任何事件驱动的各种不同的用例，这包括物联网，移动应用，基于网络的应用程序和聊天机器人等。这里简单说两个场景，方便大家思考。

### 场景一：应用负载有显著的波峰波谷

Serverless 应用成功与否的评判标准并不是公司规模的大小，而是其业务背后的具体技术问题，比如业务波峰波谷明显，如何实现削峰填谷。一个公司的业务负载具有波峰波谷时，机器资源要按照峰值需求预估；而在波谷时期机器利用率则明显下降，因为不能进行资源复用而导致浪费。

业界普遍共识是，当自有机器的利用率小于 30%，使用 Serverless 后会有显著的效率提升。对于云服务厂商，在具备了足够多的用户之后，各种波峰波谷叠加后平稳化，聚合之后资源复用性更高。比如，外卖企业负载高峰是在用餐时期，安防行业的负载高峰则是夜间，这是受各个企业业务定位所限的；而对于一个成熟的云服务厂商，如果其平台足够大，用户足够多，是不应该有明显的波峰波谷现象的。

### 场景二：典型用例 - 基于事件的数据处理

视频处理的后端系统，常见功能需求如下：视频转码、抽取数据、人脸识别等，这些均为通用计算任务，可由函数计算执行。

开发者需要自己写出实现逻辑，再将任务按照控制流连接起来，每个任务的具体执行由云厂商来负责。如此，开发变得更便捷，并且构建的系统天然高可用、实时弹性伸缩，用户不需要关心机器层面问题。

经典的基于事件驱动的服务很多都还是在各自的业务应用上处理，如之前ground日志处理时若是观察到线程数量不足时可以返回驱动业务应用增加线程或者saturn类似的定时服务以FaaS增加资源复用率或者一些公共的服务如认证鉴权也可以  以BaaS服务的形式减少重复造轮子（如CAS的部分（cas需要保持登陆状态信息）和百源） 



## Serverless架构优势

- 缩短交付时间
  Serverless架构允许开发人员在极短时间内（几小时、几天）交付新的应用程序，而不是像以前一样需要几个星期或几个月。在新的应用程序中，依赖于第三方API提供服务的例子很多，如认证(OAuth)、社交、地图、人工智能等。
- 增强可伸缩性
  所有人都希望自己开发的应用能够快速获取大量的新增用户，但是当活跃用户快速增长的时候，服务器的压力也会激增。使用Serverless架构的体系不再有上述担忧，可以及时、灵活进行扩展来应对快速增长的活跃用户带来的访问压力。
- 降低成本
  Serverless架构模式可以降低计算能力和人力资源方面的成本，如果不需要服务器，就不用花费时间重新造轮子、风险监测、图像处理，以及基础设施管理，操作成本会直线下降。
- 改善用户体验
  用户通常不太关心基础设施，而更注重于功能和用户体验。Serverless架构允许团队将资源集中在用户体验上。
- 减少延迟及优化地理位置信息
  应用规模能力取决于三个方面：用户数量、所在位置及网络延迟。当应用要面向全国甚至全球用户的时候，通常会产生较高的延迟，从而降低用户体验。在Serverless架构下，供应商在每个用户附近都有节点，大幅度降低了访问延迟，因此所有用户的体验都可以得到提升。

## Serverless 的问题

###  概述

对于企业来说，支持Serverless计算的平台可以节省大量时间和成本，同时可以释放员工，让开发者得以开展更有价值的工作，而不是管理基础设施。另一方面可以提高敏捷度，更快速地推出新应用和新服务，进而提高客户满意度。但是Serverless不是完美的，它也存在一些问题，需要慎重应用在生产环境。

###  不适合长时间运行应用

Serverless 在请求到来时才运行。这意味着，当应用不运行的时候就会进入 “休眠状态”，下次当请求来临时，应用将会需要一个启动时间，即冷启动时间。如果你的应用需要一直长期不间断的运行、处理大量的请求，那么你可能就不适合采用 Serverless 架构。如果你通过 CRON 的方式或者 CloudWatch 来定期唤醒应用，又会比较消耗资源。这就需要我们对它做优化，如果频繁调用，这个资源将会常驻内存，第一次冷启之后，就可以一直服务，直到一段时间内没有新的调用请求进来，则会转入“休眠”状态，甚至被回收，从而不消耗任何资源。

###  完全依赖于第三方服务

当你所在的企业云环境已经有大量的基础设施的时候，Serverless 对于你来说，并不是一个好东西。当我们采用某云服务厂商的 Serverless 架构时，我们就和该服务供应商绑定了，那么我们再将服务迁到别的云服务商上就没有那么容易了。

我们需要修改一下系列的底层代码，能采取的应对方案，便是建立隔离层。这意味着，在设计应用的时候，就需要隔离 API 网关、隔离数据库层，考虑到市面上还没有成熟的 ORM 工具，让你既支持Firebase，又支持 DynamoDB等等。这些也将带给我们一些额外的成本，可能带来的问题会比解决的问题多。

###  缺乏调试和开发工具

当我使用 Serverless Framework 的时候，遇到了这样的问题：缺乏调试和开发工具。后来，我发现了 serverless-offline、dynamodb-local 等一系列插件之后，问题有一些改善。然而，对于日志系统来说，这仍然是一个艰巨的挑战。

每次你调试的时候，你需要一遍又一遍地上传代码。而每次上传的时候，你就好像是在部署服务器，并不能总是快速地定位出问题在哪。后来，找了一个类似于 log4j 这样的可以分级别记录日志的 Node.js 库 winston。它可以支持 error、warn、info、verbose、debug、silly 六个不同级别的日志，再结合大数据进行日志分析过滤，才能快速定位问题。

###  构建复杂

Serverless 很便宜，但是这并不意味着它很简单。AWS Lambda的 CloudFormation配置是如此的复杂，并且难以阅读及编写（JSON 格式），虽然CloudFomation提供了Template模板，但想要使用它的话，需要创建一个Stack，在Stack中指定你要使用的Template,然后aws才会按照Template中的定义来创建及初始化资源。

而Serverless Framework的配置更加简单，采用的是 YAML 格式。在部署的时候，Serverless Framework 会根据我们的配置生成 CloudFormation 配置。然而这也并非是一个真正用于生产的配置,真实的应用场景远远比这复杂。

# 9 总结

- Service Mesh
  对于大规模部署微服务，内部服务异构程度高的场景，使用Service Mesh方案是一个不错的选择。Service Mesh实现了业务逻辑和控制的解耦，但是也带来了额外的开销，由于网络中多了一跳，增加了性能的损耗和访问的延迟。同时，由于每个服务都需要部署Sidecar, 这也会使本来就具有一定复杂度的分布式系统变得更加复杂。尤其是在实施初期，对Service Mesh的管理和运维会是一个棘手的问题。因此，当我们选择使用Service Mesh架构的时候，需要对具体的Service Mesh实现方案（例如：Istio）做好充分的技术准备和经验积累工作，方能确保方案的顺利实施。

  以大规模的

- Serverless
  在微服务与容器技术火热之后，Serverless（无服务器架构）成为新的热点，无服务器云函数可以让用户无需关心服务器的部署运营，只需开发最核心的业务逻辑，即可实现上线运营，具备分布容灾能力，可以依据负载自动扩缩容，并按照实际调用次数与时长计费。

  使用Serverless架构可以免除所有运维性操作，开发人员可以更加专注于核心业务的开发，实现快速上线和迭代，把握业务发展的节奏。**Serverless架构可以认为是对微服务和容器化的一种补充**，为用户提供了一种新的选择，来应对复杂多变的需求，尤其适合快速发展的初创型公司。

DDD的火热也是基于对微服务的业务维度的重构和解耦后更高效率的降本生效而日渐火热

基于soa的其他协议规范如soap和实现如esb相较于service mesh 的不同点（ 在某些网络功能上有重叠• 控制点分散• 策略针对特定应用程序• 不处理业务逻辑问题（如映射、转换、基于内容的路由等））等还有研究的必要

以及基于对接口减负而日渐重要的流式处理等大数据架构也愈发重要，以及如同esb会遇到的瓶颈问题，对于依赖的架构如envoy和istio有基于更好的算法、流程管理实现的架构，微服务被其他更小粒度的理念代替时或者更高的硬件水平、市场变化不需要这种开发模式时，对于代理的依赖程度带来的问题，都是需要资料学习的

